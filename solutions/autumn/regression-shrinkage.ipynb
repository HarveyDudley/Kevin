{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression: Ridge Regression and LASSO\n",
    "\n",
    "**Functions**\n",
    "\n",
    "`sklearn.linear_model.RidgeCV`, `sklearn.linear_model.LassoCV`, `sklearn.preprocessing.StandardScaler`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LassoCV, RidgeCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 49\n",
    "\n",
    "Standardize the value-weighted-market return data and the 12 industry portfolios by their standard deviation. You should _not_ remove the mean since we want to match the mean in the tracking portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vwm = pd.read_csv(\"data/VWM.csv\", index_col=\"Date\")\n",
    "vwm.index = pd.to_datetime(vwm.index, format=\"%Y%m\")\n",
    "vwm = vwm.resample(\"M\").last()\n",
    "\n",
    "industries = pd.read_csv(\"data/12_Industry_portfolios.csv\", index_col=\"Date\")\n",
    "industries.index = pd.to_datetime(industries.index, format=\"%Y%m\")\n",
    "industries = industries.resample(\"M\").last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = industries[\"1980\":\"2014\"]\n",
    "y = vwm[\"VWM\"][\"1980\":\"2014\"]\n",
    "t, p = x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "Here we use the same problem (portfolio tracking) and data that was used in the Best Subset and Stepwise regression notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scale = x.std(ddof=0)\n",
    "y_scale = y.std(ddof=0)\n",
    "std_x = x / x_scale\n",
    "std_y = y / y_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "We standardize the data by dividing by the the standard deviation. Note that we do not remove the mean since the portfolio tracking problem excludes an intercept.  The transformed data is then\n",
    "\n",
    "$$ \\tilde{X} = \\frac{X - 0}{\\hat{\\sigma}} $$\n",
    "\n",
    "where the mean has been replaced by 0 since we are _not_ recentering.\n",
    "\n",
    "**Note**: We set `ddof=0` so that the large-sample standard deviation estimator is used, \n",
    "$$\\hat{\\sigma} = \\sqrt{n^{-1} \\sum_{i=1}^n (X_i - \\bar{X})^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 50\n",
    "\n",
    "Select the optimal tuning parameter in a LASSO and estimate model parameters for the tracking error minimizing portfolio using the standardized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_cv = LassoCV(fit_intercept=False)\n",
    "lasso_cv = lasso_cv.fit(std_x, std_y)\n",
    "print(f\"Optimal alpha : {lasso_cv.alpha_}\")\n",
    "lasso_cv.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "`LassoCV` can be used to cross-validate the tuning parameter (`alpha` is scikit-learn is the same as $\\lambda$ in the notes). Here we set `fit_intercept` to `False` since the model should not include an intercept.  We then choose the tuning parameter by cross-validation and report the optimal value and the estimated coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 51\n",
    "\n",
    "Transform the estimated LASSO coefficients back to the scale of the original, non-standardized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_orig = lasso_cv.coef_ * (y_scale / x_scale)\n",
    "lasso_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "We used transformed `x` and `y` values, and so to get the parameters back to the scale of the original data, we need to multiply by the ratio of the two scales.  The invariance to affine transformations is a standard property of OLS estimators.  Note the model we estimated was\n",
    "\n",
    "$$ \\frac{Y_i}{\\hat{\\sigma}_Y} = \\sum_{j=1}^k \\beta_j \\frac{X_{i,j}}{\\hat{\\sigma}_{X_j}}  + \\epsilon _i $$\n",
    "\n",
    "so to get back to the original scale we need to multiply the estimated coefficient for regressor $j$ by\n",
    "\n",
    "$$ \\frac{\\hat{\\sigma}_Y}{\\hat{\\sigma}_{X_j}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 52\n",
    "\n",
    "Select the optimal tuning parameter in a Ridge regression and estimate model parameters for the tracking error minimizing portfolio using the standardized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv = RidgeCV(alphas=np.linspace(1, 100, 100), fit_intercept=False)\n",
    "ridge_cv = ridge_cv.fit(std_x, std_y)\n",
    "print(f\"Optimal alpha : {ridge_cv.alpha_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv = RidgeCV(alphas=np.linspace(4, 6, 2001), fit_intercept=False)\n",
    "ridge_cv = ridge_cv.fit(std_x, std_y)\n",
    "print(f\"Optimal alpha : {ridge_cv.alpha_}\")\n",
    "ridge_cv.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "Ridge regression is virtually identical with the key exception that we need to pass in an arrays of `alpha` values to check.  Here we do this in two passes where the first gets a rough estimate and the second refines this value.  Again `alpha` is the same as $\\lambda$ in the notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 53\n",
    "\n",
    "Transform the estimated Ridge regression coefficients back to the scale of the original, non-standardized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_orig = ridge_cv.coef_ * (y_scale / x_scale)\n",
    "ridge_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "The coefficient need to be rescaled to be comparable to those that we would find by OLS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 54\n",
    "\n",
    "Compare the parameter estimates from the LASSO and Ridge regression to those from OLS in a plot. Use the original, non-standardized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.api import OLS\n",
    "\n",
    "plt.rc(\"font\", size=16)\n",
    "\n",
    "ols = OLS(y, x).fit().params\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 12))\n",
    "_ = pd.DataFrame([ols, lasso_orig, ridge_orig], index=[\"OLS\", \"LASSO\", \"RIDGE\"]).T.plot(\n",
    "    kind=\"bar\", ax=ax\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "The optimal shrinkage is small and the coefficient are similar to those in OLS.  This might not be the case if the sample size was smaller or the number of regressors was larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaled_pred = x @ ridge_orig\n",
    "rescaled_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "With the rescaled coefficients, we can then compute the actual in-sample predictions with the correct scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 55\n",
    "\n",
    "Use scikit-learn to scale the standardize the data by changing the scale but not the mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "y_scaler = StandardScaler(with_mean=False)\n",
    "y_2d = pd.DataFrame(y)\n",
    "y_scaler = y_scaler.fit(y_2d)\n",
    "\n",
    "x_scaler = StandardScaler(with_mean=False)\n",
    "x_scaler = x_scaler.fit(x)\n",
    "\n",
    "std_y = y_scaler.transform(y_2d)\n",
    "std_x = x_scaler.transform(x)\n",
    "\n",
    "ridge_cv = RidgeCV(alphas=np.linspace(4, 6, 2001), fit_intercept=False)\n",
    "ridge_cv = ridge_cv.fit(std_x, std_y)\n",
    "print(f\"Optimal alpha : {ridge_cv.alpha_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "scikit-learn contains a number of alternative rescaling methods.  The basic one is `StandardScaler` which by default computes\n",
    "\n",
    "$$ \\tilde{X} = \\frac{X-\\bar{X}}{\\hat{\\sigma}_X}.$$\n",
    "\n",
    "We can set `with_mean=False` to force $\\bar{X}=0$.\n",
    "\n",
    "Here we standardize both x and y using `StandardScaler` and show the results are identical. `StandardScaler` is used by first initializing it, then fitting the coefficients to a dataset.  This computes the mean and standard deviation and lets the same scaler be applied to other data later. It also lets the scaling operation be inverted, so that we can transform some standardized value $Z$ to the same scale as the original data\n",
    "\n",
    "$$ \\ddot{X} = \\hat{\\sigma}_X Z + \\bar{X}.$$\n",
    "\n",
    "**Note** `StandardScaler` expects a 2-d input, so we need to transform the `Series` version of y to a `DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 56\n",
    "\n",
    "Use the scikit-learn scaler to compute the predicted in-sample values using the Ridge ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = ridge_cv.predict(std_x)\n",
    "rescaled_pred = y_scaler.inverse_transform(pred)\n",
    "pd.Series(rescaled_pred.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "To make predictions in the original space of the y values, we first predict using the standardized values, and then we use `inverse_transform` of the `y_scaler` to transform the predictions which are for the standardized y back to the original scale. Aside from the loss of the pandas information, we can see these are the same as the predictions above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 57\n",
    "\n",
    "Use the scalar from scikit-learn to produce out-of-sample forecasts of the two shrinkage estimators and OLS and evaluate the out-of-sample SSE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the out-of-sample data\n",
    "y_oos = vwm.loc[\"2015\":, \"VWM\"]\n",
    "x_oos = industries[\"2015\":]\n",
    "\n",
    "# Transform the out-of-sample values\n",
    "std_x_oos = x_scaler.transform(x_oos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the standardized OOS values\n",
    "pred = lasso_cv.predict(std_x_oos)\n",
    "# Ensure that pred is a 2d array with shape (nobs, 1)\n",
    "pred = pred.reshape((-1,1))\n",
    "print(pred.shape)\n",
    "# Compute OOS residuals by inverting the scale of the prediction\n",
    "resid_oos = y_oos - np.squeeze(y_scaler.inverse_transform(pred))\n",
    "lasso_oos_sse = resid_oos @ resid_oos\n",
    "lasso_oos_sse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "The scikit-learn approach makes out-of-sample prediction simple using 3 steps:\n",
    "\n",
    "1. Standardized out-of-sample x values using the `x_scaler`. **Note**: This must be the same scaler used previously.\n",
    "2. Make a prediction using the transformed out-of-sample x values.\n",
    "3. Inverse the transformation of the prediction using `y_scaler`.\n",
    "\n",
    "**Note**: We _do not_ rescale the out-of-sample y values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify and fit the model\n",
    "pred = ridge_cv.predict(std_x_oos)\n",
    "resid_oos = y_oos - np.squeeze(y_scaler.inverse_transform(pred))\n",
    "ridge_oos_sse = resid_oos @ resid_oos\n",
    "ridge_oos_sse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "Predictions from ridge regression are virtually identical. The only caveat in the code is thar Ridge likes 2d arrays, and so `pred` is 2d. This needs to be squeezed to 1d before running through `inverse_transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_pred = x_oos @ ols\n",
    "resid_oos = y_oos - ols_pred\n",
    "ols_oos_sse = resid_oos @ resid_oos\n",
    "ols_oos_sse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "For good measure we can compare to OLS.  We see that both shrinkage methods have outperformed OLS despite the small differences in coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 58\n",
    "\n",
    "Directly produce out-of-sample forecasts of the two shrinkage estimators and OLS and evaluate the out-of-sample SSE without using scikit-learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_oos = industries[\"2015\":]\n",
    "direct_pred = x_oos @ ridge_orig\n",
    "\n",
    "resid = y_oos - direct_pred\n",
    "ridge_oos_sse = resid @ resid\n",
    "ridge_oos_sse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "It is also possible to directly compute the out-of-sample predictions using the transformed regression coefficients computed previously.  When the coefficients have been transformed, the remainder of the calculation is simple.\n",
    "\n",
    "Finally, we can compare the predictions using the two methods and see they are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(direct_pred, columns=[\"Transformed Coefs\"])\n",
    "df[\"scikit Scaling\"] = y_scaler.inverse_transform(pred)\n",
    "df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
